{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":285947,"sourceType":"datasetVersion","datasetId":11496}],"dockerImageVersionId":29271,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Anomaly Detection in Blockchain System**\nThis work is aimed to enhance the working of blockchain system by automatically recognizing and filtering out anomalous activities. The targeted components of blockchain architecture are Block and Transaction for the given task. Since they are the most basic and vital aspects in the working of blockchain systems.\n#### What is Anomaly Detection?\nAnomaly detection is a technique used to identify unusual patterns that do not conform to expected behavior, called outliers. It has many applications in business, health monitoring, network traffic systems, etc.\n#### What is the Blockchain System?\n“A blockchain is a time-stamped series of an immutable record of data that is managed by a cluster of computers not owned by any single entity. Each of these blocks of data is secured and bound to each other using cryptographic principles”. In simple terms, blockchain is a chain of blocks; the words “block” and “chain” are the digital information and public database, respectively. The core components of blockchain architecture:\n1. Node — user or computer within the blockchain\n2. Transaction — smallest building block of a blockchain system\n3. Block — a data structure used for keeping a set of transactions which is distributed to all nodes in the network\n4. Chain — a sequence of blocks in a specific order\n5. Miners — specific nodes which perform the block verification process\n6. Consensus— a set of rules and arrangements to carry out blockchain operations\n\n> More information about blockchain can be found at ([https://blockgeeks.com/guides/what-is-blockchain-technology/](http://) [https://www.investopedia.com/terms/b/blockchain.asp](http://)).\n\nThis kernel organization can be described as:\n1. Importing Essential Libraries\n2. Reading and Processing data\n3. Model Building & Evaluation\n   *   Isolation Forest Algorithm\n   *   K Means Algorithm\n4. Conclusion","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Import Essential Libraries","metadata":{}},{"cell_type":"code","source":"from google.cloud import bigquery\nfrom scipy.stats.mstats import zscore\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport matplotlib as mpl\nfrom pathlib import Path\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest \nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime,tzinfo\nimport scipy, json, csv, time, pytz\nfrom pytz import timezone\nimport numpy as np\nimport pandas as pd\nseed = 135\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nimport os\nos.listdir('../input/')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:05.512783Z","iopub.execute_input":"2022-01-24T21:07:05.513225Z","iopub.status.idle":"2022-01-24T21:07:08.21243Z","shell.execute_reply.started":"2022-01-24T21:07:05.513163Z","shell.execute_reply":"2022-01-24T21:07:08.211104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Reading and Processing Data","metadata":{}},{"cell_type":"markdown","source":"> **Google BigQuery Bitcoin Blockchain Dataset** consist of two projects: Transactions and Blocks containing features like block_id, transaction_id, timestamp, input, output, etc., which updates every 10 minutes. [https://bigquery.cloud.google.com/dataset/bigquery-public-data:bitcoin_blockchain](http://)","metadata":{}},{"cell_type":"code","source":"#Connecting to Google datastore (use path to ur private key)\nos.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"../input/gcp-bitcoin-project/Bitcoin Project-615d07137267.json\"\nclient = bigquery.Client()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:11.972119Z","iopub.execute_input":"2022-01-24T21:07:11.972463Z","iopub.status.idle":"2022-01-24T21:07:11.978268Z","shell.execute_reply.started":"2022-01-24T21:07:11.972414Z","shell.execute_reply":"2022-01-24T21:07:11.97743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The query to get date, number of transactions from Google BigQuery bitcoin blockchain dataset \n# Select records from the last three years and group them with respect to date\nquery_1 = \"\"\"\nSELECT \n   DATE(TIMESTAMP_MILLIS(timestamp)) AS Date,\n   COUNT(transactions) AS Transactions\nFROM `bigquery-public-data.bitcoin_blockchain.blocks`\nGROUP BY date\nHAVING date >= '2016-08-12' AND date <= '2019-08-12'\nORDER BY date\n\"\"\"\nquery_job_1 = client.query(query_1)\n# Waits for the query to finish\niterator_1 = query_job_1.result(timeout=30)\nrows_1 = list(iterator_1)\ndf_1 = pd.DataFrame(data=[list(x.values()) for x in rows_1], columns=list(rows_1[0].keys()))","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:15.882215Z","iopub.execute_input":"2022-01-24T21:07:15.882879Z","iopub.status.idle":"2022-01-24T21:07:18.129462Z","shell.execute_reply.started":"2022-01-24T21:07:15.882811Z","shell.execute_reply":"2022-01-24T21:07:18.128332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The query to get sum of all satoshis spent each day and number of blocks\nquery_2 = \"\"\"\nSELECT\n  o.Date,\n  COUNT(o.block) AS Blocks,\n  SUM(o.output_price) AS Output_Satoshis\nFROM (\n  SELECT\n    DATE(TIMESTAMP_MILLIS(timestamp)) AS Date,\n    output.output_satoshis AS output_price,\n    block_id AS block\n  FROM\n    `bigquery-public-data.bitcoin_blockchain.transactions`,\n    UNNEST(outputs) AS output ) AS o\nGROUP BY\n  o.date\nHAVING o.date >= '2016-08-12' AND o.date <= '2019-08-12'\nORDER BY o.date, blocks\n\"\"\"\nquery_job_2 = client.query(query_2)\n# Waits for the query to finish\niterator_2 = query_job_2.result(timeout=30)\nrows_2 = list(iterator_2)\ndf_2 = pd.DataFrame(data=[list(x.values()) for x in rows_2], columns=list(rows_2[0].keys()))\n\ndf_2[\"Output_Satoshis\"]= df_2[\"Output_Satoshis\"].apply(lambda x: float(x/100000000))","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:20.961939Z","iopub.execute_input":"2022-01-24T21:07:20.962481Z","iopub.status.idle":"2022-01-24T21:07:22.594617Z","shell.execute_reply.started":"2022-01-24T21:07:20.962409Z","shell.execute_reply":"2022-01-24T21:07:22.5938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:30.740875Z","iopub.execute_input":"2022-01-24T21:07:30.741258Z","iopub.status.idle":"2022-01-24T21:07:30.763737Z","shell.execute_reply.started":"2022-01-24T21:07:30.741163Z","shell.execute_reply":"2022-01-24T21:07:30.762474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1.tail","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:54.224379Z","iopub.execute_input":"2022-01-24T21:07:54.225034Z","iopub.status.idle":"2022-01-24T21:07:54.235923Z","shell.execute_reply.started":"2022-01-24T21:07:54.224971Z","shell.execute_reply":"2022-01-24T21:07:54.23526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:07:58.354629Z","iopub.execute_input":"2022-01-24T21:07:58.355189Z","iopub.status.idle":"2022-01-24T21:07:58.365821Z","shell.execute_reply.started":"2022-01-24T21:07:58.355126Z","shell.execute_reply":"2022-01-24T21:07:58.364891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge the two dataframes\nresult = pd.merge(df_1,\n                 df_2[['Date', 'Blocks', 'Output_Satoshis']],\n                 on='Date')\nresult.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:03.325266Z","iopub.execute_input":"2022-01-24T21:08:03.325784Z","iopub.status.idle":"2022-01-24T21:08:03.355792Z","shell.execute_reply.started":"2022-01-24T21:08:03.325741Z","shell.execute_reply":"2022-01-24T21:08:03.355152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of records \nlen(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:07.868155Z","iopub.execute_input":"2022-01-24T21:08:07.86872Z","iopub.status.idle":"2022-01-24T21:08:07.874907Z","shell.execute_reply.started":"2022-01-24T21:08:07.868673Z","shell.execute_reply":"2022-01-24T21:08:07.873947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the overview of our data\nresult.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:11.141119Z","iopub.execute_input":"2022-01-24T21:08:11.141452Z","iopub.status.idle":"2022-01-24T21:08:11.16769Z","shell.execute_reply.started":"2022-01-24T21:08:11.141403Z","shell.execute_reply":"2022-01-24T21:08:11.167063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, Let's understand how the values of the features in our data are distributed. The bell-shaped curve is the best distribution of the values in the dataset. ","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(result['Blocks'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:19.013096Z","iopub.execute_input":"2022-01-24T21:08:19.013667Z","iopub.status.idle":"2022-01-24T21:08:19.441668Z","shell.execute_reply.started":"2022-01-24T21:08:19.013619Z","shell.execute_reply":"2022-01-24T21:08:19.439932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(result['Transactions'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:22.81086Z","iopub.execute_input":"2022-01-24T21:08:22.811181Z","iopub.status.idle":"2022-01-24T21:08:23.165176Z","shell.execute_reply.started":"2022-01-24T21:08:22.81113Z","shell.execute_reply":"2022-01-24T21:08:23.16398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(result['Output_Satoshis'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:28.948156Z","iopub.execute_input":"2022-01-24T21:08:28.948597Z","iopub.status.idle":"2022-01-24T21:08:29.293393Z","shell.execute_reply.started":"2022-01-24T21:08:28.948554Z","shell.execute_reply":"2022-01-24T21:08:29.292217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, mostly the features of data are following the bell-shaped curve. Now, let's plot the curves of three vital features (Transactions, Blocks, Output Satoshis) of our data with respect to the date. Is there some trend in these plots?","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nplt.style.use('ggplot')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n\ng = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Transactions', data=result, palette='Blues_d')\nplt.title('Transactions per day')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:08:58.513209Z","iopub.execute_input":"2022-01-24T21:08:58.513862Z","iopub.status.idle":"2022-01-24T21:08:59.684793Z","shell.execute_reply.started":"2022-01-24T21:08:58.513783Z","shell.execute_reply":"2022-01-24T21:08:59.68416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nplt.style.use('ggplot')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n\ng = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Blocks', data=result, palette='Blues_d')\nplt.title('Blocks per day')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:08.275548Z","iopub.execute_input":"2022-01-24T21:09:08.27591Z","iopub.status.idle":"2022-01-24T21:09:09.370958Z","shell.execute_reply.started":"2022-01-24T21:09:08.275868Z","shell.execute_reply":"2022-01-24T21:09:09.370143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = plt.subplots(figsize=(20,9))\ng = sns.lineplot(x='Date', y='Output_Satoshis', data=result, palette='BuGn_r')\nplt.title('Sum of all satoshis spent each day')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:19.380336Z","iopub.execute_input":"2022-01-24T21:09:19.38068Z","iopub.status.idle":"2022-01-24T21:09:20.461561Z","shell.execute_reply.started":"2022-01-24T21:09:19.380617Z","shell.execute_reply":"2022-01-24T21:09:20.460608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we should now get back to the previous question. Is there some trend here in the graphs? The possible answer can be not exactly. So why did we plot those curves? Often, In anomaly detection, we make use of different algorithms to determine the anomalies in the data. Mostly unsupervised learning-based anomaly detection algorithm uses outliers to detect anomalies. Yes, we will evaluate these outliers employing several anomaly detection algorithms in this notebook and will get back to these curves again at last to determine the anomalies. ","metadata":{}},{"cell_type":"code","source":"# check the relation among the features of data\nsns.set(style=\"ticks\")\nsns.pairplot(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:27.083147Z","iopub.execute_input":"2022-01-24T21:09:27.083452Z","iopub.status.idle":"2022-01-24T21:09:29.470309Z","shell.execute_reply.started":"2022-01-24T21:09:27.083405Z","shell.execute_reply":"2022-01-24T21:09:29.469216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Model Building & Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### Method 3: Long-short time memory(LSTM)","metadata":{}},{"cell_type":"code","source":"split_row = len(result) - int(0.2 * len(result))\ntrain_data = result.iloc[:split_row]\ntest_data = result.iloc[split_row:]","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:48.761793Z","iopub.execute_input":"2022-01-24T21:09:48.762353Z","iopub.status.idle":"2022-01-24T21:09:48.770411Z","shell.execute_reply.started":"2022-01-24T21:09:48.762288Z","shell.execute_reply":"2022-01-24T21:09:48.769488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:56.182171Z","iopub.execute_input":"2022-01-24T21:09:56.182547Z","iopub.status.idle":"2022-01-24T21:09:56.190998Z","shell.execute_reply.started":"2022-01-24T21:09:56.182475Z","shell.execute_reply":"2022-01-24T21:09:56.18973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:09:59.021927Z","iopub.execute_input":"2022-01-24T21:09:59.022546Z","iopub.status.idle":"2022-01-24T21:09:59.029632Z","shell.execute_reply.started":"2022-01-24T21:09:59.02248Z","shell.execute_reply":"2022-01-24T21:09:59.028441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:10:03.157285Z","iopub.execute_input":"2022-01-24T21:10:03.157932Z","iopub.status.idle":"2022-01-24T21:10:03.173445Z","shell.execute_reply.started":"2022-01-24T21:10:03.157868Z","shell.execute_reply":"2022-01-24T21:10:03.17237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chosen_col = 'Transactions'\nfig, ax = plt.subplots(1, figsize=(13, 7))\nax.plot(train_data[chosen_col], label='Train', linewidth=2)\nax.plot(test_data[chosen_col], label='Test', linewidth=2)\nax.set_ylabel('Transactions', fontsize=14)\nax.set_title('plot', fontsize=16)\nax.legend(loc='best', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:10:44.244019Z","iopub.execute_input":"2022-01-24T21:10:44.244429Z","iopub.status.idle":"2022-01-24T21:10:44.728374Z","shell.execute_reply.started":"2022-01-24T21:10:44.24436Z","shell.execute_reply":"2022-01-24T21:10:44.727307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndatacol = result.iloc[:, 1:2].values\ntraincol = train_data.iloc[:, 1:2].values\ntestcol = test_data.iloc[:, 1:2].values\n\nshaped_data = np.reshape(datacol,(-1,1))\ntrain_shaped = np.reshape(traincol, (-1,1))\ntest_shaped = np.reshape(testcol, (-1,1))\n\nsc = MinMaxScaler(feature_range=(0,1))\nsc.fit(shaped_data)\n\ntrain_scaled_data = sc.transform(shaped_data)\ntest_scaled_data = sc.transform(test_shaped)\n\"\"\"\nd = data['Transactions']\noutliers_fraction=0.05\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(d)\nd = pd.DataFrame(np_scaled)\nfrom sklearn.preprocessing import MinMaxScaler\n\ndatacol = dataset.iloc[:, 7:8].values\ntraincol = train_data.iloc[:, 7:8].values\ntestcol = test_data.iloc[:, 7:8].values\n\nshaped_data = np.reshape(datacol, (-1,1))\ntrain_shaped = np.reshape(traincol, (-1,1))\ntest_shaped = np.reshape(testcol, (-1,1))\n\nsc = MinMaxScaler(feature_range=(0,1))\nsc.fit(shaped_data)\n\ntrain_scaled_data = sc.transform(train_shaped)\ntest_scaled_data = sc.transform(test_shaped)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:14.197768Z","iopub.execute_input":"2022-01-24T21:46:14.198173Z","iopub.status.idle":"2022-01-24T21:46:14.213002Z","shell.execute_reply.started":"2022-01-24T21:46:14.198099Z","shell.execute_reply":"2022-01-24T21:46:14.2122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(datacol)\nlen(test_shaped)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:20.810258Z","iopub.execute_input":"2022-01-24T21:46:20.810613Z","iopub.status.idle":"2022-01-24T21:46:20.817236Z","shell.execute_reply.started":"2022-01-24T21:46:20.810546Z","shell.execute_reply":"2022-01-24T21:46:20.816052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X = []\ny = []\ntime_steps = 50\n\nfor i in range(time_steps, len(train_data)):\n    X.append(train_scaled_data[i-time_steps:i,0])\n    y.append(train_scaled_data[i, 0])\n\nX, y = np.array(X), np.array(y)\nX = np.reshape(X, (X.shape[0], X.shape[1], 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:29.341741Z","iopub.execute_input":"2022-01-24T21:46:29.34221Z","iopub.status.idle":"2022-01-24T21:46:29.350593Z","shell.execute_reply.started":"2022-01-24T21:46:29.342164Z","shell.execute_reply":"2022-01-24T21:46:29.349528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:33.33917Z","iopub.execute_input":"2022-01-24T21:46:33.339505Z","iopub.status.idle":"2022-01-24T21:46:33.345661Z","shell.execute_reply.started":"2022-01-24T21:46:33.339455Z","shell.execute_reply":"2022-01-24T21:46:33.344986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:41.482312Z","iopub.execute_input":"2022-01-24T21:46:41.482873Z","iopub.status.idle":"2022-01-24T21:46:41.490283Z","shell.execute_reply.started":"2022-01-24T21:46:41.482828Z","shell.execute_reply":"2022-01-24T21:46:41.489279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:44.670208Z","iopub.execute_input":"2022-01-24T21:46:44.670774Z","iopub.status.idle":"2022-01-24T21:46:44.677116Z","shell.execute_reply.started":"2022-01-24T21:46:44.670623Z","shell.execute_reply":"2022-01-24T21:46:44.676388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:47.681919Z","iopub.execute_input":"2022-01-24T21:46:47.682614Z","iopub.status.idle":"2022-01-24T21:46:47.689539Z","shell.execute_reply.started":"2022-01-24T21:46:47.682535Z","shell.execute_reply":"2022-01-24T21:46:47.688517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:51.322801Z","iopub.execute_input":"2022-01-24T21:46:51.323361Z","iopub.status.idle":"2022-01-24T21:46:51.329701Z","shell.execute_reply.started":"2022-01-24T21:46:51.323107Z","shell.execute_reply":"2022-01-24T21:46:51.328661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\n\nmodel = Sequential()\nmodel.add(LSTM(units=100, input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation('linear'))\nmodel.compile(optimizer='adam', loss='mse',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:55.788919Z","iopub.execute_input":"2022-01-24T21:46:55.789508Z","iopub.status.idle":"2022-01-24T21:46:56.132419Z","shell.execute_reply.started":"2022-01-24T21:46:55.789457Z","shell.execute_reply":"2022-01-24T21:46:56.131476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\ncallback = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1, callbacks=[callback], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:46:59.934884Z","iopub.execute_input":"2022-01-24T21:46:59.935229Z","iopub.status.idle":"2022-01-24T21:47:26.530342Z","shell.execute_reply.started":"2022-01-24T21:46:59.935178Z","shell.execute_reply":"2022-01-24T21:47:26.529292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:47:37.980641Z","iopub.execute_input":"2022-01-24T21:47:37.981187Z","iopub.status.idle":"2022-01-24T21:47:37.989546Z","shell.execute_reply.started":"2022-01-24T21:47:37.981117Z","shell.execute_reply":"2022-01-24T21:47:37.988426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.metrics import mean_absolute_error\n\nX_testing = []\ny_testing = []\n\nfor i in range(time_steps, len(test_data)):\n    X_testing.append(test_scaled_data[i-time_steps:i,0])\n    y_testing.append(test_scaled_data[i, 0])\n\nX_testing, y_testing = np.array(X_testing), np.array(y_testing)\nX_testing = np.reshape(X_testing, (X_testing.shape[0], X_testing.shape[1],1))\n\npredicted_trans = model.predict(X_testing)\ninv_trans = sc.inverse_transform(predicted_trans)\nreal_trans = np.reshape(y_testing, (-1,1))\nreal_trans = sc.inverse_transform(real_trans)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:47:47.167228Z","iopub.execute_input":"2022-01-24T21:47:47.167887Z","iopub.status.idle":"2022-01-24T21:47:47.50404Z","shell.execute_reply.started":"2022-01-24T21:47:47.167819Z","shell.execute_reply":"2022-01-24T21:47:47.502978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, figsize=(13, 7))\nax.plot(real_trans, label='Real', linewidth=2)\nax.plot(inv_trans, label='Pred', linewidth=2)\nax.set_ylabel('Transactions', fontsize=14)\nax.set_title('', fontsize=16)\nax.legend(loc='best', fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:47:57.463864Z","iopub.execute_input":"2022-01-24T21:47:57.464461Z","iopub.status.idle":"2022-01-24T21:47:57.902819Z","shell.execute_reply.started":"2022-01-24T21:47:57.464297Z","shell.execute_reply":"2022-01-24T21:47:57.901694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#               Jusqu'au là je m'arrête un petit moment","metadata":{}},{"cell_type":"markdown","source":"splitting the data into training and validation","metadata":{}},{"cell_type":"markdown","source":"#### Method 1: Isolation Forest","metadata":{}},{"cell_type":"code","source":"# select the three most important features (Transactions, Blocks, Output Satoshis) from the data\ndata = result[['Output_Satoshis','Blocks','Transactions']]\noutliers_fraction=0.05\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n\n# train isolation forest\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(data) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111,projection='3d')\nX = result.iloc[:,1:4].values\ncolors = np.array(['red', 'blue'])\ny_pred = model.fit_predict(data)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], s=25, color=colors[(y_pred + 1) // 2] )\nax.legend()\n#plt.xlabel('Transactions')\n#plt.ylabel('Blocks')\n#plt.zlabel('Sum of Output Satoshis')\nplt.title('Transactions vs Blocks vs Sum of Output Satoshis: Red represents Anomalies')\nplt.savefig('IsolationForest_anomaly.png', dpi=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot cites Transactions, Features, Sum of Output Satoshis features to represents the anomalies in the data using the Isolation Forest method. Now, summarize the predictions just performed by assigning them binary values i.e., 0 or 1 (0 for normal, 1 for anomaly).","metadata":{}},{"cell_type":"code","source":"# create a new column for storing the results of Isolation Forest method\nresult['anomaly_IsolationForest'] = pd.Series(model.predict(data))\nresult['anomaly_IsolationForest'] = result['anomaly_IsolationForest'].apply(lambda x: x == -1)\nresult['anomaly_IsolationForest'] = result['anomaly_IsolationForest'].astype(int)\nresult['anomaly_IsolationForest'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 38 anomalies predicted by Isolation Forest method in the last three years records. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Transactions'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Transactions'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Transactions')\nplt.savefig('IsolationForest_anomaly_Transactions.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Blocks'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Blocks'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Blocks')\nplt.savefig('IsolationForest_anomaly_Blocks.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_IsolationForest'] == 1]\nax.plot(result['Output_Satoshis'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Output_Satoshis'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Isolation Forest\")\nplt.xlabel('Date')\nplt.ylabel('Sum of Output Satoshis')\nplt.savefig('IsolationForest_anomaly_Output_Satoshis.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We just inherited use of those three previous plots (Transactions, Blocks, Output Satoshis vs. date) for better understanding the Isolation Forest method predicted anomalies by visualizing them with outliers anomaly. ","metadata":{}},{"cell_type":"markdown","source":"#### Method 2: K-Means","metadata":{}},{"cell_type":"code","source":"# This code has been taken from kernel https://github.com/anish-saha/EventDetection-Paradigm01/blob/master/KMeans.ipynb\n\ndef pca_results(good_data, pca):\n    # Dimension indexing\n    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n    # PCA components\n    components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n    components.index = dimensions\n\n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n\n    # Create a bar plot visualization\n    fig, ax = plt.subplots(figsize = (10,10))\n\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)\n\n\n    # Display the explained variance ratios\n    for i, ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n %.4f\"%(ev))\n\n    # Return a concatenated DataFrame\n    return pd.concat([variance_ratios, components], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ = data.copy() # make a copy of data with three already selected features\ndata_ = data_.reset_index(drop=True)\n\ndata_[:] = MinMaxScaler().fit_transform(data_[:])\npca = PCA(n_components=2) # we have selected 2 components in PCA for simplicity\npca.fit(data_)\nreduced_data = pca.transform(data_)\nreduced_data = pd.DataFrame(reduced_data)\n\nnum_clusters = range(1, 20)\n\nkmeans = [KMeans(n_clusters=i, random_state=seed).fit(reduced_data) for i in num_clusters]\nscores = [kmeans[i].score(reduced_data) for i in range(len(kmeans))]\n\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot(num_clusters, scores, linewidth = 4)\nplt.xticks(num_clusters)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above elbow-curve helps to determine the number of clusters for the K-means algorithm. The most drastic change recorded in the elbow-curve at 3 or 4 or 5 number of clusters. So let's which one of them is better for our case. ","metadata":{}},{"cell_type":"code","source":"correlations = pd.DataFrame(data=data_).corr()\npca_results(correlations, pca)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Choosing the three clusters based on the elbow curve\nbest_num_cluster__ = 3\nkm__ = KMeans(n_clusters=best_num_cluster__, random_state=seed)\nkm__.fit(reduced_data)\nkm__.predict(reduced_data)\nlabels__1 = km__.labels_\n\n#Choosing the four clusters based on the elbow curve\nbest_num_cluster = 4\nkm = KMeans(n_clusters=best_num_cluster, random_state=seed)\nkm.fit(reduced_data)\nkm.predict(reduced_data)\nlabels = km.labels_\n\n#Choosing the five clusters based on the elbow curve\nbest_num_cluster_ = 5\nkm_ = KMeans(n_clusters=best_num_cluster_, random_state=seed)\nkm_.fit(reduced_data)\nkm_.predict(reduced_data)\nlabels_1 = km_.labels_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting based on three cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels__1.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 3 clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting based on four cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 4 clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting based on five cluster\nfig = plt.figure(1, figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=labels_1.astype(np.float), edgecolor=\"k\", s=16)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clusters based on K means: 5 clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best results were drawn with a plot with four clusters. So, we will proceed further by considering four clusters for the problem.","metadata":{}},{"cell_type":"code","source":"reduced_data.loc[0]\nmod = kmeans[best_num_cluster-1]\nmod.cluster_centers_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_data['Principal Component 1'] = reduced_data[0]\nreduced_data['Principal Component 2'] = reduced_data[1]\nreduced_data.drop(columns = [0, 1], inplace=True)\nreduced_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getDistanceByPoint(data, model):\n    distance = []\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.append(np.linalg.norm(Xa-Xb))\n    return distance\n\noutliers_fraction = 0.05\n# find the distance between each point and its nearest centroid. The largest distances will be consdiered anomalies\ndistance = getDistanceByPoint(reduced_data, kmeans[best_num_cluster-1])\ndistance = pd.Series(distance)\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n\n\n# anomaly_kmeans contain the anomaly result of the above method  (0:normal, 1:anomaly) \nresult['anomaly_kmeans'] = (distance >= threshold).astype(int)\n\n# visualisation of anomaly with cluster view\n#fig, ax = plt.subplots(figsize=(10,6))\ncolors = {0:'blue', 1:'red'}\n#colors = {1:'#f70505', 0:'#0a48f5'}\nplt.figure(figsize=(7,7))\nplt.scatter(reduced_data.iloc[:,0], reduced_data.iloc[:,1], \n            c=result[\"anomaly_kmeans\"].apply(lambda x: colors[x]), s=25)\nplt.xlabel('principal feature 1')\nplt.ylabel('principal feature 2')\nplt.title('Anomaly prediction using KMeans: Red represents Anomaly')\nplt.savefig('KMeans_anomaly.png', dpi=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result['anomaly_kmeans'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 38 anomalies predicted by K-Means algorithm in the last three years records. Now, similarly plot the outlier anomaly with respect to Transactions, Blocks, Sum of Output Satoshis. Do not forget to compare the previous model result with the current model results. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Transactions'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Transactions'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Transactions')\nplt.savefig('KMeans_anomaly_Transactions.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Blocks'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Blocks'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Blocks')\nplt.savefig('KMeans_anomaly_Blocks.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\n\n#anomaly\na = result.loc[result['anomaly_kmeans'] == 1]\nax.plot(result['Output_Satoshis'], color='black', label = 'Normal', linewidth=1.5)\nax.scatter(a.index ,a['Output_Satoshis'], color='red', label = 'Anomaly', s=16)\nplt.legend()\nplt.title(\"Anamoly Detection Using Kmeans\")\nplt.xlabel('Date')\nplt.ylabel('Sum of Output Satoshis')\nplt.savefig('KMeans_anomaly_Output_Satoshis.png', dpi=1000)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final result dataframe\nresult.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the cases for final anomaly in which both the algorithms predicted anomaly\nfinal_anomaly = result.query('anomaly_kmeans == 1 & anomaly_IsolationForest == 1')\nfinal_anomaly.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the cases in which either of the two algorithms predicted anomaly\npossible_anomaly = result.query('anomaly_kmeans == 1 | anomaly_IsolationForest == 1')\npossible_anomaly.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the cases where no algorithm predicted anomaly\nno_anomaly = result.query('anomaly_kmeans == 0 & anomaly_IsolationForest == 0')\nno_anomaly.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_anomaly = len(final_anomaly)+len(possible_anomaly)\npercent_total_anomaly = total_anomaly*100/len(result)\nprint('Total records:',len(result))\nprint('Number of final anomaly:', len(final_anomaly))\nprint('Number of possible anomaly:', len(possible_anomaly))\nprint('Total anomaly:', total_anomaly)\nprint('Percentage of total anomaly in the data: %0.2f' % percent_total_anomaly)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Method 3: LSTM**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"Hence, we have determined 76 possible anomalies in the last three years (i.e., 10 percent anomaly) of Google BigQuery Bitcoin Blockchain dataset. We have utilized two popular anomaly detection methods: Isolation Forest and K-Means. The notable fact obtained from the results is that both anomaly detection methods predicts 38 anomaly cases, which is reasonably consistent. We can further explore the anomaly detection in blockchain systems by utilizing more data or including other features. Lastly, anomaly detection tools can be effectively used to make the blockchain systems even stronger and safer by automatically recognizing and filtering out anomalous activities. ","metadata":{}}]}